# Battleship (Amiral Battƒ±) - Deep Q-Learning AI

Bu proje, Deep Q-Learning (DQN) kullanarak kendi kendini eƒüitebilen bir Amiral Battƒ± AI'sƒ± i√ßerir. AI, self-play y√∂ntemiyle kendine kar≈üƒ± oynayarak strateji √∂ƒürenir.

## üéÆ √ñzellikler

### AI Yetenekleri
- **Deep Q-Network (DQN)**: 5 katmanlƒ± derin sinir aƒüƒ±
- **Stratejik Karar Verme**: Olasƒ±lƒ±k haritalarƒ± ve pattern tanƒ±ma
- **Self-Play Eƒüitim**: Kendine kar≈üƒ± oynayarak √∂ƒürenme
- **Experience Replay**: Ge√ßmi≈ü deneyimlerden √∂ƒürenme
- **Target Network**: Stabilize edilmi≈ü √∂ƒürenme

### AI'nƒ±n √ñƒürendiƒüi Stratejiler
1. **Akƒ±llƒ± Atƒ±≈ü Se√ßimi**: Gemi olasƒ±lƒ±ƒüƒ± y√ºksek b√∂lgeleri hedefleme
2. **Pattern Tanƒ±ma**: Ardƒ±≈üƒ±k vuru≈ülarƒ± takip etme ve hat olu≈üturma
3. **Olasƒ±lƒ±k Hesaplama**: Her kare i√ßin gemi bulunma olasƒ±lƒ±ƒüƒ±nƒ± hesaplama
4. **√áevresel Farkƒ±ndalƒ±k**: Vuru≈ü sonrasƒ± kom≈üu kareleri √∂nceliklendirme
5. **Boyut Tahmini**: Kalan gemi boyutlarƒ±na g√∂re strateji belirleme

## üì¶ Kurulum

### Gereksinimler
```bash
pip install -r requirements.txt
```

Veya manuel kurulum:
```bash
pip install pygame torch numpy matplotlib
```

## üöÄ Kullanƒ±m

### 1. Ana Oyunu √áalƒ±≈ütƒ±rma
```bash
python battleship_ai_dqn.py
```

#### Oyun Kontrolleri
- **Mouse**: Gemi yerle≈ütirme ve atƒ±≈ü yapma
- **R**: Gemi y√∂n√ºn√º deƒüi≈ütir (yatay/dikey)
- **N**: Yeni oyun ba≈ülat
- **T**: Training modunu a√ß/kapa
- **S**: Modeli kaydet

### 2. AI'yƒ± Eƒüitme (Self-Play)

#### Hƒ±zlƒ± Eƒüitim (500 oyun)
```bash
python train_self_play.py
```

#### √ñzel Sayƒ±da Oyunla Eƒüitim
```bash
python train_self_play.py train 1000  # 1000 oyun
python train_self_play.py train 5000  # 5000 oyun
```

#### Sadece Test Etme
```bash
python train_self_play.py test
```

## üß† AI Mimarisi

### Neural Network Yapƒ±sƒ±
```
Input Layer (320 features)
    ‚Üì
Hidden Layer 1 (256 neurons) + BatchNorm + ReLU + Dropout
    ‚Üì
Hidden Layer 2 (512 neurons) + BatchNorm + ReLU + Dropout
    ‚Üì
Hidden Layer 3 (512 neurons) + BatchNorm + ReLU + Dropout
    ‚Üì
Hidden Layer 4 (256 neurons) + BatchNorm + ReLU + Dropout
    ‚Üì
Output Layer (100 Q-values)
```

### State Representation (320 features)
1. **Tahta Kanallarƒ± (300)**: 
   - Hit channel (100)
   - Miss channel (100)
   - Unknown channel (100)

2. **Ekstra √ñzellikler (20)**:
   - Heat map (ƒ±sƒ± haritasƒ±) istatistikleri
   - Toplam vuru≈ü/iska/bilinmeyen sayƒ±larƒ±
   - Ardƒ±≈üƒ±k vuru≈ü pattern'leri
   - Olasƒ±lƒ±k haritasƒ± istatistikleri
   - Kenar ve k√∂≈üe analizleri

### Reward System
```python
# Vuru≈ü: +10
# Gemi batƒ±rma: +50 + (gemi_boyu * 20)
# Iska: -2
# Ardƒ±≈üƒ±k vuru≈ü bonusu: +5
# Oyun kazanma: +100
# Oyun kaybetme: -50
```

## üìä Training ƒ∞statistikleri

Eƒüitim sƒ±rasƒ±nda ≈üu metrikler takip edilir:
- **Win Rate**: Kazanma oranƒ±
- **Hit Rate**: ƒ∞sabet oranƒ±
- **Game Length**: Ortalama oyun uzunluƒüu
- **Epsilon**: Ke≈üif oranƒ± (exploration rate)

Grafikler otomatik olarak `training_progress_[timestamp].png` olarak kaydedilir.

## üéØ Performans Beklentileri

### Eƒüitim A≈üamalarƒ±

1. **0-100 Oyun**: Rastgele atƒ±≈ülar, temel pattern √∂ƒürenme
2. **100-500 Oyun**: Vuru≈ü takibi, basit stratejiler
3. **500-1000 Oyun**: Olasƒ±lƒ±k hesaplama, geli≈ümi≈ü stratejiler
4. **1000+ Oyun**: Optimizasyon, ince ayar

### Beklenen Sonu√ßlar (1000 oyun sonrasƒ±)
- Random AI'ya kar≈üƒ± kazanma oranƒ±: >%85
- Ortalama isabet oranƒ±: >%40
- Ortalama oyun s√ºresi: <60 hamle

## üîß Hyperparameter Tuning

`battleship_ai_dqn.py` dosyasƒ±nda deƒüi≈ütirilebilir parametreler:

```python
LEARNING_RATE = 0.001      # √ñƒürenme hƒ±zƒ±
GAMMA = 0.95               # Discount factor
EPSILON_START = 1.0        # Ba≈ülangƒ±√ß ke≈üif oranƒ±
EPSILON_END = 0.01         # Minimum ke≈üif oranƒ±
EPSILON_DECAY = 0.995      # Ke≈üif azalma oranƒ±
BATCH_SIZE = 32            # Batch boyutu
MEMORY_SIZE = 10000        # Experience replay bellek boyutu
TARGET_UPDATE = 100        # Target network g√ºncelleme sƒ±klƒ±ƒüƒ±
```

## üìù Model Dosyalarƒ±

- `battleship_dqn_model.pth`: Eƒüitilmi≈ü model weights
- `training_stats_[episode].json`: Eƒüitim istatistikleri
- `training_progress_[timestamp].png`: ƒ∞lerleme grafikleri

## üö¶ Training ƒ∞pu√ßlarƒ±

1. **ƒ∞lk Eƒüitim**: En az 500-1000 oyunla ba≈ülayƒ±n
2. **ƒ∞teratif Eƒüitim**: Modeli y√ºkleyip √ºzerine eƒüitmeye devam edebilirsiniz
3. **Overfitting Kontrol√º**: Hit rate √ßok y√ºksekse (%60+) overfitting olabilir
4. **Exploration**: Epsilon deƒüeri √ßok hƒ±zlƒ± d√º≈ü√ºyorsa EPSILON_DECAY'i artƒ±rƒ±n

## üéÆ Oyun ƒ∞√ßi AI Davranƒ±≈ülarƒ±

AI ≈üu davranƒ±≈ülarƒ± sergiler:

1. **Hunt Mode**: Sistematik arama (checkerboard pattern)
2. **Target Mode**: Vuru≈ü sonrasƒ± kom≈üu karelere odaklanma
3. **Line Extension**: Ardƒ±≈üƒ±k vuru≈ülarƒ± hatta devam ettirme
4. **Probability Mapping**: Olasƒ± gemi konumlarƒ±nƒ± hesaplama
5. **Smart Recovery**: Ba≈üarƒ±sƒ±z pattern sonrasƒ± yeni strateji

## üêõ Bilinen Sorunlar ve √á√∂z√ºmler

1. **CUDA Hatasƒ±**: CPU kullanmak i√ßin `device = torch.device("cpu")` yapƒ±n
2. **Bellek T√ºkenmesi**: MEMORY_SIZE'ƒ± d√º≈ü√ºr√ºn
3. **Yava≈ü Eƒüitim**: BATCH_SIZE'ƒ± d√º≈ü√ºr√ºn veya TARGET_UPDATE'i artƒ±rƒ±n

## üìà Gelecek ƒ∞yile≈ütirmeler

- [ ] Multi-agent training (3+ AI)
- [ ] Prioritized experience replay
- [ ] Dueling DQN architecture
- [ ] Noisy networks for exploration
- [ ] Curriculum learning (kolay ‚Üí zor)
- [ ] Transfer learning for different board sizes
- [ ] Real-time visualization of AI thinking

## üìú Lisans

Bu proje eƒüitim ama√ßlƒ±dƒ±r. √ñzg√ºrce kullanabilir ve deƒüi≈ütirebilirsiniz.

## ü§ù Katkƒ±da Bulunma

ƒ∞yile≈ütirme √∂nerileriniz varsa, l√ºtfen pull request g√∂nderin veya issue a√ßƒ±n!

---

**Not**: ƒ∞lk eƒüitim biraz zaman alabilir. Sabƒ±rlƒ± olun, AI zamanla geli≈üecektir! üöÄ
